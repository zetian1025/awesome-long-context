# awesome-long-context

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/zetian1025/awesome-long-context) [![License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

## Efficient Inference, Sparse Attention, Efficient KV Cache

#### [2020/01] [Reformer: The Efficient Transformer](https://arxiv.org/pdf/2001.04451.pdf)

#### [2020/06] [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/pdf/2006.04768.pdf)

#### [2022/12] [Parallel Context Windows for Large Language Models](https://aclanthology.org/2023.acl-long.352.pdf)

#### [2023/04] [Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering](https://arxiv.org/pdf/2304.12102.pdf)

#### [2023/05] [Landmark Attention: Random-Access Infinite Context Length for Transformers](https://arxiv.org/pdf/2305.16300.pdf)

#### [2023/05] [Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time](https://arxiv.org/pdf/2305.17118.pdf)

#### [2023/06] [Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time](https://openreview.net/pdf?id=wIPIhHd00i)

#### [2023/06] [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/pdf/2306.14048.pdf)

#### [2023/07] [Scaling In-Context Demonstrations with Structured Attention](https://arxiv.org/pdf/2307.02690.pdf)

#### [2023/08] [LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models](https://arxiv.org/pdf/2308.16137.pdf)

#### [2023/09] [EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS](https://arxiv.org/pdf/2309.17453.pdf)

#### [2023/10] [HyperAttention: Long-context Attention in Near-Linear Time](https://arxiv.org/pdf/2310.05869.pdf)

#### [2023/10] [TRAMS: Training-free Memory Selection for Long-range Language Modeling](https://arxiv.org/pdf/2310.15494.pdf)

## External Memory & Information Retrieval
#### [2023/06] [Augmenting Language Models with Long-Term Memory](https://arxiv.org/pdf/2306.07174.pdf)

#### [2023/06] [Long-range Language Modeling with Self-retrieval](https://arxiv.org/pdf/2306.13421.pdf)

#### [2023/07] [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/pdf/2307.03170.pdf)


## Positional Encoding
#### [2021/04] [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864.pdf)

#### [2022/03] [Transformer Language Models without Positional Encodings Still Learn Positional Information](https://aclanthology.org/2022.findings-emnlp.99.pdf)

#### [2022/04] [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://openreview.net/forum?id=R8sQPpGCv0)

#### [2022/05] [KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation](http://arxiv.org/pdf/2205.09921.pdf)

#### [2022/12] [Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis](https://arxiv.org/pdf/2212.10356.pdf)

#### [2022/12] [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/pdf/2212.06713.pdf)

#### [2023/05] [Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings](https://arxiv.org/pdf/2305.13571.pdf)

#### [2023/06] [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/pdf/2306.15595.pdf)

#### [2023/07] [Exploring Transformer Extrapolation](https://arxiv.org/pdf/2307.10156.pdf)

#### [2023/09] [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/pdf/2309.00071.pdf)

#### [2023/09] [Effective Long-Context Scaling of Foundation Models](https://arxiv.org/pdf/2309.16039.pdf)

#### [2023/10] [CLEX: Continuous Length Extrapolation for Large Language Models](https://arxiv.org/pdf/2310.16450.pdf)


## Context Compression
#### [2022/12] [Structured Prompting: Scaling In-Context Learning to 1,000 Examples](https://arxiv.org/pdf/2212.06713.pdf)

#### [2023/05] [Efficient Prompting via Dynamic In-Context Learning](https://arxiv.org/pdf/2305.11170.pdf)

#### [2023/05] [Adapting Language Models to Compress Contexts](https://arxiv.org/pdf/2305.14788.pdf)

#### [2023/05] [Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers](https://arxiv.org/pdf/2305.15805.pdf)

#### [2023/07] [In-context Autoencoder for Context Compression in a Large Language Model](https://arxiv.org/pdf/2307.06945.pdf)

#### [2023/10] [Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs](https://arxiv.org/pdf/2310.01801.pdf)

#### [2023/10] [RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation](https://arxiv.org/pdf/2310.04408.pdf)

#### [2023/10] [Compressing Context to Enhance Inference Efficiency of Large Language Models](https://arxiv.org/pdf/2310.06201.pdf)

#### [2023/10] [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/pdf/2310.05736.pdf)

#### [2023/10] [LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression](https://arxiv.org/pdf/2310.06839.pdf)

#### [2023/10] [TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction](https://arxiv.org/pdf/2310.15556.pdf)

## Architecture Variances
#### [2021/11] [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/pdf/2111.00396.pdf)

#### [2022/12] [Hungry Hungry Hippos: Towards Language Modeling with State Space Models](https://arxiv.org/pdf/2212.14052.pdf)

#### [2023/02] [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/pdf/2302.10866.pdf)

#### [2023/04] [Scaling Transformer to 1M tokens and beyond with RMT](https://arxiv.org/pdf/2304.11062.pdf)

#### [2023/06] [Block-State Transformer](https://arxiv.org/pdf/2306.09539.pdf)

#### [2023/07] [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/pdf/2307.08621.pdf)

#### [2023/10] [Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors](https://openreview.net/forum?id=PdaPky8MUn)

#### [2023/10] [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://openreview.net/forum?id=AL1fq05o7H)

## White-Box
#### [2019/06] [Theoretical Limitations of Self-Attention in Neural Sequence Models](https://aclanthology.org/2020.tacl-1.11.pdf)

#### [2020/06] [$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers](https://arxiv.org/pdf/2006.04862.pdf)

#### [2022/02] [Overcoming a Theoretical Limitation of Self-Attention](https://aclanthology.org/2022.acl-long.527.pdf)

#### [2023/05] [Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer](https://arxiv.org/pdf/2305.16380.pdf)

#### [2023/10] [JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention](https://arxiv.org/pdf/2310.00535.pdf)

## Long Context Modeling
#### [2023/07] [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/pdf/2307.02486.pdf)

#### [2023/08] [Giraffe: Adventures in Expanding Context Lengths in LLMs](https://arxiv.org/pdf/2308.10882.pdf)

#### [2023/09] [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/pdf/2309.12307.pdf)

#### [2023/10] [Mistral 7B](https://arxiv.org/pdf/2310.06825.pdf)

## Benchmarks
#### [2020/11] [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/pdf/2011.04006.pdf)

#### [2022/01] [SCROLLS: Standardized CompaRison Over Long Language Sequences](https://arxiv.org/pdf/2201.03533.pdf)

#### [2023/01] [LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization](https://arxiv.org/pdf/2301.13298.pdf)

#### [2023/05] [ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding](https://arxiv.org/pdf/2305.14196.pdf)

#### [2023/08] [LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding](https://arxiv.org/pdf/2308.14508.pdf)

#### [2023/10] [M4LE: A MULTI-ABILITY MULTI-RANGE MULTITASK MULTI-DOMAIN LONG-CONTEXT EVALUATION BENCHMARK FOR LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2310.19240v1.pdf)

## Data

#### [2023/12] [Structured Packing in LLM Training Improves Long Context Utilization](https://arxiv.org/abs/2312.17296)

#### [2024/01] [LongAlign: A Recipe for Long Context Alignment of Large Language Models](https://arxiv.org/abs/2401.18058)

#### [2024/02] [Data Engineering for Scaling Language Models to 128K Context](https://arxiv.org/abs/2402.10171)

## Others
#### [2023/07] [Zero-th Order Algorithm for Softmax Attention Optimization](https://arxiv.org/pdf/2307.08352.pdf)

#### [2023/10] [(Dynamic) Prompting might be all you need to repair Compressed LLMs](https://arxiv.org/pdf/2310.00867.pdf)

#### [2023/10] [Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors](https://arxiv.org/pdf/2310.02980.pdf)
